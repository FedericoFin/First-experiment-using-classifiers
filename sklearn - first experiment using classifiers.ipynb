{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First program where I  will be using different types of classifiers in order to predict unknown datapoints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and the classifiers that we are going to use.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB    # importo GaussianNB dal modulo sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model  # not a classifier, but interesting to compare outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the training dataset (X, y) as well as the test set X \"punti_sconosciuti\"\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])   # training features\n",
    "y = np.array([1, 1, 1, 2, 2, 2])    # trainig labels\n",
    "\n",
    "punti_sconosciuti = [[-0.8, -1],[-0.4, -0.5],[2, 2.6]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1) Using Naive Bayes Model is [1 1 2]\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()  # uso GaussianNB per creare il classifier clf\n",
    "clf.fit(X, y)  # dò in pasto i training sets al classifier, ed esso impara i patterns\n",
    "\n",
    "# creiamo il vettore di predizioni \"pred\" usando la funzione \"predict\" sul clf, con cui passiamo le features da testare\n",
    "pred = clf.predict(punti_sconosciuti) \n",
    "\n",
    "print(\"\\n1) Using Naive Bayes Model is \" + str(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Method 'partial_fit' is used do make incremental fit on a batch of samples.\n",
    "This method is expected to be called several times consecutively on different chunks of a dataset \n",
    "so as to implement out-of-core or online learning.\n",
    "This is especially useful when the whole dataset is too big to fit in memory at once.\n",
    "'''\n",
    "\n",
    "clf_pf = GaussianNB()\n",
    "clf_pf.partial_fit(X, y, np.unique(y))\n",
    "print(clf_pf.predict([[-0.6, -0.3]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "[2]\n"
     ]
    }
   ],
   "source": [
    "# Other predictions\n",
    "print(clf.predict([[4, 4]]))\n",
    "print(clf.predict([[40, 40]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Accuracy of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# misuriamo l'accuracy della prediction usando accuracy_score, \n",
    "# fornendo anche i label reali dei punti sconosciuti per confrontarli con la predizione\n",
    "\n",
    "label_punti_sconosciuti = [1, 2, 2]\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# con \"accuracy_score\" confrontiamo pred con i valori veri dei label dei punti sconosciuti\n",
    "print (accuracy_score(pred, label_punti_sconosciuti))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "# test dell'accuracy \"terra terra\" con ciclo for\n",
    "\n",
    "numero_elementi = len(label_punti_sconosciuti)\n",
    "\n",
    "a=0\n",
    "for i in range(numero_elementi):\n",
    "    a = a+(label_punti_sconosciuti[i] == pred[i])\n",
    "\n",
    "accuracy = a/numero_elementi\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2) Using Decision Tree Prediction is [1 1 2]\n"
     ]
    }
   ],
   "source": [
    "# Then we fit the training data and predict in this style: {Decision Tree Model}\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc = clf.fit(X,y)\n",
    "print(\"\\n2) Using Decision Tree Prediction is \" + str(dtc.predict(punti_sconosciuti)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K Nearest Neighbors Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) Using K Neighbors Classifier Prediction is [1 1 2]\n"
     ]
    }
   ],
   "source": [
    "#{KN Neighbors Classifier}\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X,y)\n",
    "print(\"3) Using K Neighbors Classifier Prediction is \" + str(knn.predict(punti_sconosciuti)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4) Using MLPC Classifier Prediction is [1 1 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fede\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#{using MLPClassifier}\n",
    "mlpc = MLPClassifier()\n",
    "mlpc.fit(X,y)\n",
    "print(\"4) Using MLPC Classifier Prediction is \" + str(mlpc.predict(punti_sconosciuti)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5) Using RandomForestClassifier Prediction is [1 1 2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#{using Random Forest}\n",
    "rfor = RandomForestClassifier()\n",
    "rfor.fit(X,y)\n",
    "print(\"5) Using RandomForestClassifier Prediction is \" + str(rfor.predict(punti_sconosciuti)) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6) Using Linear Regression Prediction is [1.16666667 1.33333333 2.36666667]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# (not a classifier, but interesting to compare outputs)\n",
    "# Usando la regressione lineave avrò però delle predizioni continue e non categoriche (quindi non sarà 1 o 2)\n",
    "lr = linear_model.LinearRegression()\n",
    "lr.fit(X, y)\n",
    "print(\"6) Using Linear Regression Prediction is \" + str(lr.predict(punti_sconosciuti)) +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also use the train_test_split method to authomatize the break of the dataset in training and test pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[190, 70, 44], [166, 65, 45]] ['male', 'male']\n",
      "['male' 'male']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We start with training data. In this example we have a set of vectors (height, weight, shoe size) and the class \n",
    "XX = [[190,70,44],[166,65,45],[190,90,47],[175,64,39],[171,75,40],[177,80,42],[160,60,38],[144,54,37]]\n",
    "yy = ['male','male','male','male','female','female','female','female']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(XX, yy, test_size=0.2)\n",
    "\n",
    "print(X_test, y_test)   # datapoint che andremo a far predire al modello\n",
    "\n",
    "c = GaussianNB()  # uso GaussianNB per creare il classifier clf\n",
    "clf.fit(X_train, y_train)  # dò in pasto i training sets al classifier, ed esso impara i patterns\n",
    "\n",
    "# creiamo il vettore di predizioni \"prediction\" usando la funzione \"predict\" sul clf, con cui passiamo le features da testare\n",
    "prediction = clf.predict(X_test) \n",
    "\n",
    "print(prediction) # presizione del modello\n",
    "\n",
    "print(accuracy_score(prediction, y_test))  # accuracy della predizione"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Misurare l'accurcy con medoto cross validation\n",
    "##### Si tratta di dividere il dataset in k parti, e a iterazione si traina il modello su k-1 parti e lo si testa sulla parte non usata nel training, e a iterazione si usa ogni singola parte per il test (quindi se k = 10 si divide dataset in 10 parti, si traina il modello sulle prime 9 e si testa sulla decima, poi si traina il dataset sulle parti dalla 1 alla 8 + la 10 e si testa sulla parte 9, e così via). Per grandi datasets si consiglia k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.  0.5 0.5 0.5]\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(cross_val_score(clf, XX, yy, cv=4))\n",
    "print(cross_val_score(clf, XX, yy, cv=4).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
